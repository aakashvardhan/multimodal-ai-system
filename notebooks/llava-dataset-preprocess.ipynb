{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1373,"status":"ok","timestamp":1728808661714,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"},"user_tz":-240},"id":"X7xKguX6Wr4T","outputId":"78e13eef-2be4-4517-ae53-008e3c5d6aa4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'multimodal-ai-system'...\n","remote: Enumerating objects: 35, done.\u001b[K\n","remote: Counting objects: 100% (35/35), done.\u001b[K\n","remote: Compressing objects: 100% (29/29), done.\u001b[K\n","remote: Total 35 (delta 4), reused 34 (delta 3), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (35/35), 761.43 KiB | 23.79 MiB/s, done.\n","Resolving deltas: 100% (4/4), done.\n","/content/multimodal-ai-system\n"]}],"source":["import os\n","\n","repo_url = \"https://github.com/aakashvardhan/multimodal-ai-system.git\"\n","local_dir = \"/content/multimodal-ai-system\"\n","\n","# Check if the local directory already exists\n","if not os.path.exists(local_dir):\n","    # Clone the repository because it does not exist\n","    !git clone {repo_url}\n","    %cd {local_dir}\n","else:\n","    # Change directory to the local repository\n","    %cd {local_dir}\n","    # Pull the latest changes because the repository already exists\n","    !git pull\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22387,"status":"ok","timestamp":1728808685010,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"},"user_tz":-240},"id":"Jl6JeU2PXVpw","outputId":"8eec9f6f-ce56-4387-d217-7c614f175bc1"},"outputs":[{"name":"stdout","output_type":"stream","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.8/313.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q -r requirements.txt"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1274453,"status":"ok","timestamp":1728809959456,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"},"user_tz":-240},"id":"fanRkzcvXmDj","outputId":"ffa16d0a-426d-4d10-daa5-920f24eb19be"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-10-13 08:38:04--  https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json\n","Resolving huggingface.co (huggingface.co)... 18.67.181.126, 18.67.181.124, 18.67.181.36, ...\n","Connecting to huggingface.co (huggingface.co)|18.67.181.126|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-lfs.hf.co/repos/4d/41/4d41ea1e2709f0e68e9e361e4218192b9620c5a3f2cb8055bc625942b6cd3039/6b68bc5ca2bfd8a71119af0e8454929668ccda6a334955ccc95d114fc8d082fa?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llava_instruct_150k.json%3B+filename%3D%22llava_instruct_150k.json%22%3B\u0026response-content-type=application%2Fjson\u0026Expires=1729067884\u0026Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyOTA2Nzg4NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80ZC80MS80ZDQxZWExZTI3MDlmMGU2OGU5ZTM2MWU0MjE4MTkyYjk2MjBjNWEzZjJjYjgwNTViYzYyNTk0MmI2Y2QzMDM5LzZiNjhiYzVjYTJiZmQ4YTcxMTE5YWYwZTg0NTQ5Mjk2NjhjY2RhNmEzMzQ5NTVjY2M5NWQxMTRmYzhkMDgyZmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__\u0026Signature=BSgMdPH3ynBfRHh3W7gPwbR9Edn4CjKSbe8c-JtRwTekFi3o3D7-dFVhRZFeXZIK0AIrePgOEV3JPmC5zJ4EMZUarkeQAVk8okJ73c68oQdRWrkZvzRjAFoZQyxGG7PZFDynrinfGC4G8tsDKIaCsglMUVNqFmyaPXMrA6Wbj3hJqbEh6nn2rxg9vhGsz8ySbg6F3HNctyM2ueRQzcV7lHFxfJvrT-WXX9%7ECNbGBPvAUkwHKMd0DITvxgBKpvXL3w1IHcBkBgf2v26uRbCMkmlg-hm94QbJdQvFDh-l3KKfOs6uvieeBkIMe8pKvfcTiP3fu9nMD-0i-PR3ilaPjUw__\u0026Key-Pair-Id=K3RPWS32NSSJCE [following]\n","--2024-10-13 08:38:04--  https://cdn-lfs.hf.co/repos/4d/41/4d41ea1e2709f0e68e9e361e4218192b9620c5a3f2cb8055bc625942b6cd3039/6b68bc5ca2bfd8a71119af0e8454929668ccda6a334955ccc95d114fc8d082fa?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llava_instruct_150k.json%3B+filename%3D%22llava_instruct_150k.json%22%3B\u0026response-content-type=application%2Fjson\u0026Expires=1729067884\u0026Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyOTA2Nzg4NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80ZC80MS80ZDQxZWExZTI3MDlmMGU2OGU5ZTM2MWU0MjE4MTkyYjk2MjBjNWEzZjJjYjgwNTViYzYyNTk0MmI2Y2QzMDM5LzZiNjhiYzVjYTJiZmQ4YTcxMTE5YWYwZTg0NTQ5Mjk2NjhjY2RhNmEzMzQ5NTVjY2M5NWQxMTRmYzhkMDgyZmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__\u0026Signature=BSgMdPH3ynBfRHh3W7gPwbR9Edn4CjKSbe8c-JtRwTekFi3o3D7-dFVhRZFeXZIK0AIrePgOEV3JPmC5zJ4EMZUarkeQAVk8okJ73c68oQdRWrkZvzRjAFoZQyxGG7PZFDynrinfGC4G8tsDKIaCsglMUVNqFmyaPXMrA6Wbj3hJqbEh6nn2rxg9vhGsz8ySbg6F3HNctyM2ueRQzcV7lHFxfJvrT-WXX9%7ECNbGBPvAUkwHKMd0DITvxgBKpvXL3w1IHcBkBgf2v26uRbCMkmlg-hm94QbJdQvFDh-l3KKfOs6uvieeBkIMe8pKvfcTiP3fu9nMD-0i-PR3ilaPjUw__\u0026Key-Pair-Id=K3RPWS32NSSJCE\n","Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 65.8.11.102, 65.8.11.7, 65.8.11.35, ...\n","Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|65.8.11.102|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 228941895 (218M) [application/json]\n","Saving to: ‘llava_instruct_150k.json’\n","\n","llava_instruct_150k 100%[===================\u003e] 218.34M   252MB/s    in 0.9s    \n","\n","2024-10-13 08:38:05 (252 MB/s) - ‘llava_instruct_150k.json’ saved [228941895/228941895]\n","\n","--2024-10-13 08:38:05--  http://images.cocodataset.org/zips/train2017.zip\n","Resolving images.cocodataset.org (images.cocodataset.org)... 16.182.107.209, 52.217.67.28, 52.216.92.3, ...\n","Connecting to images.cocodataset.org (images.cocodataset.org)|16.182.107.209|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 19336861798 (18G) [application/zip]\n","Saving to: ‘train2017.zip’\n","\n","train2017.zip       100%[===================\u003e]  18.01G  17.3MB/s    in 19m 22s \n","\n","2024-10-13 08:57:28 (15.9 MB/s) - ‘train2017.zip’ saved [19336861798/19336861798]\n","\n"]}],"source":["!wget -c https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json\n","\n","!wget -c http://images.cocodataset.org/zips/train2017.zip\n","!unzip -q train2017.zip -d coco/"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1165,"status":"ok","timestamp":1728809960610,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"},"user_tz":-240},"id":"jEJh2NO2iCHD","outputId":"f732d408-da6a-44f4-fdb9-41d0b92633cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Token is valid (permission: write).\n","Your token has been saved in your configured git credential helpers (store).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import login\n","\n","login(\n","  token=\"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\",\n","  add_to_git_credential=True\n",")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1208,"status":"ok","timestamp":1728810440859,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"},"user_tz":-240},"id":"fFQzEvwk5yfC"},"outputs":[],"source":["prompt = \"\"\"Describe the differences between classical and quantum computing.\"\"\"\n","\n","system_message = \"You are a helpful multimodal assistant.\""]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":532,"status":"ok","timestamp":1728810448158,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"},"user_tz":-240},"id":"VCByaVQTIYjm","outputId":"08645354-5026-4bb2-a3c1-4a97a69cac6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/multimodal-ai-system\n"]}],"source":["%cd multimodal-ai-system/"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10447756,"status":"ok","timestamp":1728820901123,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"},"user_tz":-240},"id":"Bhpp2Egl7RRl","outputId":"2b241244-fbdf-4a18-810f-8fa498acec94"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","100%|██████████| 157712/157712 [2:52:32\u003c00:00, 15.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processed 81479 images. Embeddings saved to llava_image_embeddings.json\n","Image to index mapping saved to image_to_index_mapping.json\n"]}],"source":["import os\n","import json\n","import torch\n","from PIL import Image\n","from tqdm import tqdm\n","from datasets import load_dataset\n","from transformers import CLIPProcessor, CLIPModel\n","import numpy as np\n","\n","# Load CLIP model and processor\n","model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n","\n","# Set device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","\n","# Load Instruct 150k dataset from local JSON file\n","dataset = load_dataset('json', data_files='llava_instruct_150k.json', split='train')\n","\n","# Function to get image embedding\n","def get_image_embedding(image_path):\n","    try:\n","        image = Image.open(image_path).convert('RGB')\n","        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n","        with torch.no_grad():\n","            image_features = model.get_image_features(**inputs)\n","        return image_features.cpu().numpy().squeeze()\n","    except Exception as e:\n","        print(f\"Error processing {image_path}: {str(e)}\")\n","        return None\n","\n","# Process images and store embeddings\n","embeddings = {}\n","coco_image_dir = 'coco/train2017'  # Update this path to your COCO train2017 directory\n","\n","for item in tqdm(dataset):\n","    image_filename = item['image']\n","    image_path = os.path.join(coco_image_dir, image_filename)\n","\n","    if os.path.exists(image_path):\n","        embedding = get_image_embedding(image_path)\n","        if embedding is not None:\n","            embeddings[image_filename] = embedding.tolist()  # Convert to list for JSON serialization\n","    else:\n","        print(f\"Image not found: {image_path}\")\n","\n","# Save embeddings to file\n","with open('llava_image_embeddings.json', 'w') as f:\n","    json.dump(embeddings, f)\n","\n","print(f\"Processed {len(embeddings)} images. Embeddings saved to llava_image_embeddings.json\")\n","\n","# Optional: Save a mapping of image filenames to dataset indices\n","image_to_index = {item['image']: idx for idx, item in enumerate(dataset)}\n","with open('image_to_index_mapping.json', 'w') as f:\n","    json.dump(image_to_index, f)\n","\n","print(\"Image to index mapping saved to image_to_index_mapping.json\")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1728822099335,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"},"user_tz":-240},"id":"19RpZf8J06nU","outputId":"41c0656d-53ba-406f-901b-f32213b0b9e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;34masset\u001b[0m/  image_to_index_mapping.json  \u001b[01;34mnotebooks\u001b[0m/        \u001b[01;34msrc\u001b[0m/\n","\u001b[01;34mcoco\u001b[0m/   llava_image_embeddings.json  README.md         train2017.zip\n","\u001b[01;34mextra\u001b[0m/  llava_instruct_150k.json     requirements.txt\n"]}],"source":["%ls"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25597,"status":"ok","timestamp":1728822240266,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"},"user_tz":-240},"id":"dYcrl4oB_JFX","outputId":"9c604c95-d010-4dd5-f19f-50b1f1cbee16"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of embeddings: 81479\n","Sample image filenames: ['000000033471.jpg', '000000052846.jpg', '000000334872.jpg', '000000319154.jpg', '000000398214.jpg']\n","\n","Embedding for index 0 (image: 000000033471.jpg):\n","Shape: (768,)\n","First 5 values: [ 1.046507    1.39119172 -0.01748174  0.80124754 -0.93853086]\n","Last 5 values: [ 0.31271049  0.0622136   0.25745347  0.07590292 -0.68126208]\n","\n","Type of embeddings: \u003cclass 'dict'\u003e\n","Type of first embedding: \u003cclass 'list'\u003e\n","Length of first embedding: 768\n"]}],"source":["import json\n","import numpy as np\n","\n","def load_json(file_path):\n","    try:\n","        with open(file_path, 'r') as f:\n","            return json.load(f)\n","    except FileNotFoundError:\n","        print(f\"File not found: {file_path}\")\n","        return None\n","    except json.JSONDecodeError:\n","        print(f\"Error decoding JSON from file: {file_path}\")\n","        return None\n","\n","def get_embedding_by_index(embeddings, index):\n","    if isinstance(embeddings, dict):\n","        if index \u003c len(embeddings):\n","            image_filename = list(embeddings.keys())[index]\n","            return np.array(embeddings[image_filename]), image_filename\n","    return None, f\"Index {index} out of range\"\n","\n","def get_embedding_by_filename(embeddings, filename):\n","    if filename in embeddings:\n","        return np.array(embeddings[filename]), filename\n","    return None, f\"Image {filename} not found in embeddings\"\n","\n","# Main execution\n","embeddings_file = 'llava_image_embeddings.json'\n","embeddings = load_json(embeddings_file)\n","\n","if embeddings is None:\n","    print(\"Failed to load embeddings. Exiting.\")\n","    exit()\n","\n","# Print some information about the embeddings\n","print(f\"Total number of embeddings: {len(embeddings)}\")\n","print(f\"Sample image filenames: {list(embeddings.keys())[:5]}\")\n","\n","# Try to access by index\n","index_to_view = 0  # Change this to the index you want to view\n","embedding, result = get_embedding_by_index(embeddings, index_to_view)\n","\n","if embedding is not None:\n","    print(f\"\\nEmbedding for index {index_to_view} (image: {result}):\")\n","    print(f\"Shape: {embedding.shape}\")\n","    print(f\"First 5 values: {embedding[:5]}\")\n","    print(f\"Last 5 values: {embedding[-5:]}\")\n","else:\n","    print(f\"\\nError accessing by index: {result}\")\n","\n","    # If index access fails, try accessing by filename\n","    print(\"\\nTrying to access by filename...\")\n","    sample_filename = list(embeddings.keys())[0]\n","    embedding, result = get_embedding_by_filename(embeddings, sample_filename)\n","\n","    if embedding is not None:\n","        print(f\"Embedding for image {result}:\")\n","        print(f\"Shape: {embedding.shape}\")\n","        print(f\"First 5 values: {embedding[:5]}\")\n","        print(f\"Last 5 values: {embedding[-5:]}\")\n","    else:\n","        print(f\"Error accessing by filename: {result}\")\n","\n","# Print data type information\n","print(f\"\\nType of embeddings: {type(embeddings)}\")\n","if len(embeddings) \u003e 0:\n","    first_key = list(embeddings.keys())[0]\n","    print(f\"Type of first embedding: {type(embeddings[first_key])}\")\n","    print(f\"Length of first embedding: {len(embeddings[first_key])}\")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1074,"status":"ok","timestamp":1728822647674,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"},"user_tz":-240},"id":"zGpCjyAS1-MY","outputId":"30fbac42-9f6c-4480-ccda-320441b513d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["81479\n"]}],"source":["%ls pt_embeddings | wc -l"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47100,"status":"ok","timestamp":1728822771264,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"},"user_tz":-240},"id":"h_XtTg8d23z8","outputId":"b91ec20d-6edb-43ac-9775-40c3506a23c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":575,"status":"ok","timestamp":1728822930412,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"},"user_tz":-240},"id":"sCjbv-ss302Q","outputId":"f9c0f71c-89c6-4ada-956f-86bd4831fffb"},"outputs":[{"name":"stdout","output_type":"stream","text":[" \u001b[0m\u001b[01;34mFastSAM\u001b[0m/                \u001b[01;34ms17-transformers\u001b[0m/                      \u001b[01;34m'S24 Car Game'\u001b[0m/\n"," \u001b[01;34mhf_spaces\u001b[0m/              \u001b[01;34ms18-transformer-speeding-up-strategy\u001b[0m/  'S24 Car Game.zip'\n"," \u001b[01;34mllm-finetuning\u001b[0m/         \u001b[01;34ms19-bulding-gpt\u001b[0m/                        \u001b[01;34ms24-stable-diffusion\u001b[0m/\n"," \u001b[01;34mmultimodal-system\u001b[0m/      \u001b[01;34ms20-tokenizers\u001b[0m/                         \u001b[01;34ms25-simulated-crawler\u001b[0m/\n"," \u001b[01;34mpytorch-tutorial\u001b[0m/       \u001b[01;34ms21-classnotes\u001b[0m/                         \u001b[01;34ms7-in-depth-practice\u001b[0m/\n"," \u001b[01;34ms10-residual-ocp\u001b[0m/       \u001b[01;34ms21-gpt2\u001b[0m/                               \u001b[01;34ms8-normalization\u001b[0m/\n"," \u001b[01;34ms11-gradcam\u001b[0m/            \u001b[01;34ms22-notes\u001b[0m/                              \u001b[01;34ms9-advanced-conv-data-aug\u001b[0m/\n"," \u001b[01;34ms12-yolov3\u001b[0m/             \u001b[01;34ms22-unet-vae\u001b[0m/                          \u001b[01;34m'Session 2'\u001b[0m/\n"," \u001b[01;34ms13-lightning-gradio\u001b[0m/   \u001b[01;34ms22-variational-autoencoders\u001b[0m/          \u001b[01;34m'Session 4'\u001b[0m/\n"," \u001b[01;34ms15-yolov9\u001b[0m/             \u001b[01;34ms23-fast-sam\u001b[0m/                          \u001b[01;34m'Session 5'\u001b[0m/\n"," \u001b[01;34ms15-yolov9-project\u001b[0m/     \u001b[01;34ms23-tiny-clip\u001b[0m/                         \u001b[01;34m'Session 6'\u001b[0m/\n"]}],"source":["%ls /content/drive/MyDrive/ERA\\ v2"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":4076,"status":"ok","timestamp":1728827744368,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"},"user_tz":-240},"id":"rDuDq6723UA-"},"outputs":[],"source":["!mv /content/multimodal-ai-system/llava_image_embeddings.json /content/drive/MyDrive/ERA\\ v2/multimodal-system/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1TwvvqLEM6Hwlo4x25QGpXz5V6TTIwGD0"},"id":"x1t4dr1qFHwj","outputId":"6b8be0fa-c4f7-4373-96e7-9d23f473a30a"},"outputs":[],"source":["!cat /content/drive/MyDrive/ERA\\ v2/multimodal-system/llava_image_embeddings.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1u2plnGNbie"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMdQx8l8pp/LoZ+L0AEEtnS","gpuType":"T4","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}