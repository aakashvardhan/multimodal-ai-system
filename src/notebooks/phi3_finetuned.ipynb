{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 15 13:45:35 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   25C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "instruct_dataset = f'../llava_instruct_150k.json'\n",
    "with open(instruct_dataset, 'r') as f:\n",
    "    instruct_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '000000033471',\n",
       "  'image': '000000033471.jpg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': '<image>\\nWhat are the colors of the bus in the image?'},\n",
       "   {'from': 'gpt', 'value': 'The bus in the image is white and red.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'What feature can be seen on the back of the bus?'},\n",
       "   {'from': 'gpt', 'value': 'The back of the bus features an advertisement.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'Is the bus driving down the street or pulled off to the side?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The bus is driving down the street, which is crowded with people and other vehicles.'}]},\n",
       " {'id': '000000052846',\n",
       "  'image': '000000052846.jpg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': 'Where is the cat positioned in the image?\\n<image>'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The cat is positioned on top of the back of the couch in the living room.'},\n",
       "   {'from': 'human', 'value': 'What is the cat doing in the image?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The cat is coming out from some curtains onto the couch and is sitting or standing on top of it.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'What is the color of the curtains near the cat?'},\n",
       "   {'from': 'gpt', 'value': 'The color of the curtains near the cat is red.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'Besides the cat, what else can be seen on the couch?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'There are lots of pillows on the couch besides the cat.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'What kind of room does the couch appear to be in?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The couch appears to be in a living room setting.'}]},\n",
       " {'id': '000000334872',\n",
       "  'image': '000000334872.jpg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': '<image>\\nAre the people in the image skiing downhill or cross-country skiing?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The people in the image are cross-country skiing in the woods, as they are skiing on a trail rather than a steep slope.'},\n",
       "   {'from': 'human', 'value': 'How many people are in the image?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'There are two people in the image, both on skis in the snow.'},\n",
       "   {'from': 'human', 'value': 'What kind of environment are they skiing in?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'They are skiing in a wooded environment, following a trail through the trees while surrounded by snow.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'Do the skiers have any additional gear with them besides their skis and poles?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'Yes, the two male skiers are carrying backpacks while they ski through the woods. The backpacks might contain essentials for their skiing adventure, such as food, water, extra clothing, or safety equipment.'}]},\n",
       " {'id': '000000319154',\n",
       "  'image': '000000319154.jpg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': '<image>\\nWhat color is the dog in the image?'},\n",
       "   {'from': 'gpt', 'value': 'The dog in the image is brown.'},\n",
       "   {'from': 'human', 'value': 'What type of collar does the dog have?'},\n",
       "   {'from': 'gpt', 'value': 'The dog has a red collar.'},\n",
       "   {'from': 'human', 'value': \"What is the dog's position behind the window?\"},\n",
       "   {'from': 'gpt', 'value': 'The dog is sitting behind the window.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'How does the dog appear to feel as it looks out the window?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'As the dog stares out the window, it appears to feel longingly, perhaps suggesting a desire to be outside or interacting with someone or something that has caught its attention.'},\n",
       "   {'from': 'human', 'value': \"Can you describe the dog's fur?\"},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The dog has shaggy, fuzzy, and furry brown fur, which makes it look cute and cuddly.'}]},\n",
       " {'id': '000000398214',\n",
       "  'image': '000000398214.jpg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': 'What type of sign and traffic device can be seen in the image?\\n<image>'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'In the image, there is a street sign and a traffic light above a city road.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'How many traffic lights are visible in the image?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'There are a total of six traffic lights visible in the image.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'What is the current color of the traffic lights in the image?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'All the traffic lights in the image are showing the color red at the same time.'},\n",
       "   {'from': 'human', 'value': 'What kind of setting is the image taken in?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The image is taken in a big city setting with a city street, an overpass, and buildings in the distance.'},\n",
       "   {'from': 'human', 'value': 'Are there any cars visible in the image?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'Yes, there are cars visible on the city street in the image.'}]},\n",
       " {'id': '000000520873',\n",
       "  'image': '000000520873.jpg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': '<image>\\nWhat is the girl eating in the image?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The girl in the image is eating a dessert, which appears to be a graham cracker treat or a cookie sandwich.'},\n",
       "   {'from': 'human', 'value': \"Describe the girl's hair color and clothing.\"},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The girl has blonde hair, and she is wearing a pink shirt.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'What color is the plate that the dessert is on?'},\n",
       "   {'from': 'gpt', 'value': 'The dessert is on a green plate.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'Is the girl looking at the camera or focusing on her dessert?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The girl is looking up at the camera while taking a bite of her dessert.'},\n",
       "   {'from': 'human', 'value': 'Where is the girl eating her dessert?'},\n",
       "   {'from': 'gpt', 'value': 'The girl is eating her dessert at the table.'}]},\n",
       " {'id': '000000575173',\n",
       "  'image': '000000575173.jpg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': '<image>\\nWhat type of aircraft is shown in the image?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The image shows a large passenger jet belonging to China Airlines.'},\n",
       "   {'from': 'human', 'value': 'Is the airplane in the air or on the ground?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The airplane is on the ground, as it is seen taxiing down the runway next to a body of water.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'What is the location of the harbor or body of water in relation to the airplane?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The body of water is located right next to the runway where the airplane is taxiing.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'What type of landscape is visible around the airplane?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The landscape visible around the airplane consists of a runway and a large body of water or lake located next to it.'}]},\n",
       " {'id': '000000087286',\n",
       "  'image': '000000087286.jpg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': 'What is hanging from the traffic light pole?\\n<image>'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'A green street sign is hanging from the traffic light pole.'},\n",
       "   {'from': 'human', 'value': 'How many stoplights are on the light post?'},\n",
       "   {'from': 'gpt', 'value': 'There are two stoplights on the light post.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'What is the background of the image where the street sign is situated?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The street sign on the pole is set against a backdrop of a daytime sky.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'What can you infer about the location of the street sign?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The location of the street sign in the picture is an urban or suburban setting, likely at an intersection. The presence of the traffic light pole and stoplights suggests that the street sign is meant to guide motorists and pedestrians in navigating the area. The daytime sky in the background indicates that the photo was captured during daylight hours, which can provide better visibility for those trying to read the street sign.'}]},\n",
       " {'id': '000000032286',\n",
       "  'image': '000000032286.jpg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': '<image>\\nHow many chocolate-covered doughnuts are there in the image?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'There are seven chocolate-covered doughnuts in the image.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'What is the color of the frosting on the doughnuts?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The frosting on the doughnuts is chocolate-colored.'},\n",
       "   {'from': 'human', 'value': 'Where are the doughnuts placed?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The doughnuts are placed in a white box, which is sitting on top of a counter.'},\n",
       "   {'from': 'human', 'value': 'What is the arrangement of the doughnuts?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The doughnuts are sitting together on a tray inside the white box.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'What type of counter is the box of doughnuts placed on?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'Based on the image, it is not possible to definitively determine the type of counter the box of doughnuts is placed on, as the focus is on the doughnuts themselves. The counter could be in a bakery, a supermarket, a cafe, or any other location where doughnuts are being sold or displayed.'}]},\n",
       " {'id': '000000175217',\n",
       "  'image': '000000175217.jpg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': 'What is the color of the sink and vanity in the bathroom?\\n<image>'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The sink and vanity in the bathroom are beige in color.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'What is the color of the toilet in the bathroom?'},\n",
       "   {'from': 'gpt', 'value': 'The toilet in the bathroom is white.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'Is there a painting hanging on the wall in the bathroom?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'Yes, there is a painting hanging on the wall in the bathroom.'},\n",
       "   {'from': 'human',\n",
       "    'value': 'What type of fixtures or counters are used in the bathroom?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'Marble fixtures or counters are used in the bathroom.'},\n",
       "   {'from': 'human', 'value': 'In general, how is the bathroom decorated?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The bathroom is well-decorated with artwork hanging on the wall and earth-toned tile and paint. The combination of a beige sink and vanity, white toilet, and marble counters or fixtures creates an elegant and inviting atmosphere in the bathroom. The presence of a painting on the wall adds a personal touch and contributes to the overall aesthetic of the space.'}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find '/home/ubuntu/multimodal-system/notebooks/data/llava_instruct_150k.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Usage example\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 48\u001b[0m     preprocessor \u001b[38;5;241m=\u001b[39m \u001b[43mImageCaptionPreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Get a few sample captions\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     samples \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mget_sample_captions(\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m, in \u001b[0;36mImageCaptionPreprocessor.__init__\u001b[0;34m(self, dataset_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/llava_instruct_150k.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/load.py:2074\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2069\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2070\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2071\u001b[0m )\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2074\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/load.py:1795\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1793\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1794\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1795\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1804\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1808\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/load.py:1551\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1542\u001b[0m \n\u001b[1;32m   1543\u001b[0m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(filename):\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/load.py:935\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m base_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[1;32m    930\u001b[0m patterns \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    931\u001b[0m     sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files)\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns(base_path, download_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config)\n\u001b[1;32m    934\u001b[0m )\n\u001b[0;32m--> 935\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mDataFilesDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m supports_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m _MODULE_SUPPORTS_METADATA\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m supports_metadata \u001b[38;5;129;01mand\u001b[39;00m patterns \u001b[38;5;241m!=\u001b[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/data_files.py:721\u001b[0m, in \u001b[0;36mDataFilesDict.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    716\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    718\u001b[0m     out[key] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    719\u001b[0m         patterns_for_key\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[0;32m--> 721\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDataFilesList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m     )\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/data_files.py:624\u001b[0m, in \u001b[0;36mDataFilesList.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         data_files\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 624\u001b[0m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         )\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/datasets/data_files.py:411\u001b[0m, in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find '/home/ubuntu/multimodal-system/notebooks/data/llava_instruct_150k.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from typing import List, Dict\n",
    "\n",
    "class ImageCaptionPreprocessor:\n",
    "    def __init__(self, dataset_path: str = \"data\"):\n",
    "        self.dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "        self.train_data = self.dataset['train']\n",
    "\n",
    "    def extract_captions(self) -> List[Dict[str, str]]:\n",
    "        captions = []\n",
    "        for item in self.train_data:\n",
    "            conversation = item['conversations']\n",
    "            image_id = item['id']\n",
    "            \n",
    "            # Extract the caption (human's first message)\n",
    "            caption = self._extract_caption_from_conversation(conversation)\n",
    "            \n",
    "            if caption:\n",
    "                captions.append({\n",
    "                    'image_id': image_id,\n",
    "                    'caption': caption\n",
    "                })\n",
    "        \n",
    "        return captions\n",
    "\n",
    "    def _extract_caption_from_conversation(self, conversation: List[Dict[str, str]]) -> str:\n",
    "        # The caption is typically the first human message\n",
    "        for turn in conversation:\n",
    "            if turn['from'] == 'human':\n",
    "                # Remove any prefixes like \"Human: \" if present\n",
    "                caption = turn['value'].lstrip(\"Human: \").strip()\n",
    "                return caption\n",
    "        return \"\"\n",
    "\n",
    "    def save_captions_to_file(self, output_file: str = \"data/preprocessed_captions.json\"):\n",
    "        captions = self.extract_captions()\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(captions, f, indent=2)\n",
    "        print(f\"Saved {len(captions)} captions to {output_file}\")\n",
    "\n",
    "    def get_sample_captions(self, n: int = 5) -> List[Dict[str, str]]:\n",
    "        captions = self.extract_captions()\n",
    "        return captions[:n]\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessor = ImageCaptionPreprocessor()\n",
    "    \n",
    "    # Get a few sample captions\n",
    "    samples = preprocessor.get_sample_captions(5)\n",
    "    print(\"Sample captions:\")\n",
    "    for sample in samples:\n",
    "        print(f\"Image ID: {sample['image_id']}\")\n",
    "        print(f\"Caption: {sample['caption']}\\n\")\n",
    "    \n",
    "    # Save all captions to a file\n",
    "    preprocessor.save_captions_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
